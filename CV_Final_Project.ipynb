{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNGWUeDoiwiu/cXece0ThY0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#IMPORTS\n","\n","!pip install mediapipe opencv-python\n","\n","import mediapipe as mp\n","import cv2\n","import numpy as np\n","import uuid\n","import os\n","from matplotlib import pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","from mediapipe import solutions as mp_solutions\n","mp_drawing = mp.solutions.drawing_utils\n","mp_holistic = mp.solutions.holistic\n","\n","# Unmount the drive if it was previously mounted\n","!fusermount -u /content/drive\n","# Remove any existing files in the mount point\n","!rm -rf /content/drive\n","#mounting the drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rjYl11L_4JgY","executionInfo":{"status":"ok","timestamp":1734550280719,"user_tz":300,"elapsed":44086,"user":{"displayName":"Rima Rebei","userId":"16806725013394065055"}},"outputId":"1c35772e-44dd-4f07-a67d-7ed772080901"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mediapipe\n","  Downloading mediapipe-0.10.20-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n","Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n","Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.8.0)\n","Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n","Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.25.5)\n","Collecting sounddevice>=0.4.4 (from mediapipe)\n","  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.2.0)\n","Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n","Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.1)\n","Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\n","Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.55.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (11.0.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n","Downloading mediapipe-0.10.20-cp310-cp310-manylinux_2_28_x86_64.whl (35.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n","Installing collected packages: sounddevice, mediapipe\n","Successfully installed mediapipe-0.10.20 sounddevice-0.5.1\n","fusermount: failed to unmount /content/drive: No such file or directory\n","Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":30,"metadata":{"id":"nwMzw5V84DAf","executionInfo":{"status":"ok","timestamp":1734555466653,"user_tz":300,"elapsed":605,"user":{"displayName":"Rima Rebei","userId":"16806725013394065055"}}},"outputs":[],"source":["# Initialize MediaPipe Hands model\n","mp_hands = mp_solutions.hands\n","mp_drawing = mp_solutions.drawing_utils\n","\n","### DEPTH IMAGE DATA\n","\n","#extract timestamp from filename\n","def extract_timestamp(filename):\n","    \"\"\"Extract timestamp from file name.\"\"\"\n","    try:\n","        parts = filename.split('_')\n","        if len(parts) > 1:\n","            return parts[1].split('.')[0]\n","        return None\n","    except IndexError:\n","        return None\n","\n","#preprocess depth images\n","def preprocess_depth_image(depth_image, min_depth=10, max_depth=1000):\n","    \"\"\"Preprocess depth image by applying filtering and normalization.\"\"\"\n","    #normalize depth values to [0, 1]\n","    depth_image_normalized = cv2.normalize(depth_image, None, alpha=0, beta=1,\n","                                           norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n","\n","    #remove noisy values outside the valid depth range --> huge issue with noisy data\n","    depth_image_filtered = np.where((depth_image > min_depth) & (depth_image < max_depth), depth_image, 0)\n","\n","    #apply median filtering to smooth noise\n","    depth_image_smoothed = cv2.medianBlur(depth_image_filtered.astype(np.uint8), ksize=5)\n","\n","    #fill missing regions using inpainting\n","    mask = (depth_image_smoothed == 0).astype(np.uint8)\n","    depth_image_inpainted = cv2.inpaint(depth_image_smoothed, mask, inpaintRadius=5, flags=cv2.INPAINT_TELEA)\n","\n","    #apply bilateral filtering to refine edges\n","    depth_image_bilateral = cv2.bilateralFilter(depth_image_inpainted, d=9, sigmaColor=75, sigmaSpace=75)\n","\n","    return depth_image_bilateral\n","\n","def match_rgb_and_depth_images(folder_path): #images\n","    \"\"\"Match RGB and depth IMAGES by timestamps\"\"\"\n","    file_names = os.listdir(folder_path)\n","    rgb_files = sorted([f for f in file_names if f.startswith('rgbFrame')])\n","    depth_files = sorted([f for f in file_names if f.startswith('depthFrame')])\n","\n","    matched_frames = {}\n","    for rgb_file in rgb_files:\n","        timestamp = extract_timestamp(rgb_file)\n","        if timestamp:\n","            matched_frames[timestamp] = {'rgb': rgb_file, 'depth': None}\n","\n","    for depth_file in depth_files:\n","        timestamp = extract_timestamp(depth_file)\n","        if timestamp in matched_frames:\n","            matched_frames[timestamp]['depth'] = depth_file\n","\n","    return {k: v for k, v in matched_frames.items() if v['rgb'] and v['depth']}\n","\n","\n","\n","#validate landmarks using depth data\n","def validate_landmarks(rgb_image, depth_image, hand_landmarks):\n","    \"\"\"Validate hand landmarks using depth data.\"\"\"\n","    h, w = depth_image.shape\n","    for landmark in hand_landmarks.landmark:\n","        cx, cy = int(landmark.x * w), int(landmark.y * h)\n","        if not (0 <= cx < w and 0 <= cy < h and depth_image[cy, cx] > 0):\n","            return False\n","    return True\n","\n","#compute angles between three points\n","def compute_angle(a, b, c):\n","    \"\"\"Compute the angle formed by three points.\"\"\"\n","    ab = np.array(b) - np.array(a)\n","    bc = np.array(c) - np.array(b)\n","    cos_theta = np.dot(ab, bc) / (np.linalg.norm(ab) * np.linalg.norm(bc))\n","    angle = np.degrees(np.arccos(np.clip(cos_theta, -1.0, 1.0)))\n","    return angle\n","\n","def compute_vector_angle(a, b, c):\n","    \"\"\"\n","    Compute the angle between vectors ab and bc using the dot product.\n","    Parameters:\n","        a, b, c: 3D coordinates of points (list or ndarray).\n","    \"\"\"\n","    ab = np.array(b) - np.array(a)\n","    bc = np.array(c) - np.array(b)\n","    cos_theta = np.dot(ab, bc) / (np.linalg.norm(ab) * np.linalg.norm(bc))\n","    return np.degrees(np.arccos(np.clip(cos_theta, -1.0, 1.0)))\n","\n","def compute_abduction_distance(point1, point2):\n","    \"\"\"\n","    Compute the Euclidean distance between two points.\n","    Parameters:\n","        point1, point2: 3D coordinates of points.\n","    \"\"\"\n","    return np.linalg.norm(np.array(point1) - np.array(point2))\n","\n","def process_frames_and_compute_angles(folder_path, matched_frames, finger_indices, finger_colors, use_preprocessed=True):\n","    \"\"\"\n","    Process RGB and depth frames, compute joint angles, and visualize 3D hand poses.\n","    Uses functions from the referenced paper to calculate flexion and abduction angles.\n","    \"\"\"\n","    hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)\n","\n","    for i, (timestamp, files) in enumerate(matched_frames.items()):\n","        rgb_path = os.path.join(folder_path, files['rgb'])\n","        depth_path = os.path.join(folder_path, files['depth'])\n","\n","        # Load RGB and depth images\n","        rgb_image = cv2.imread(rgb_path)\n","        depth_image = cv2.imread(depth_path, cv2.IMREAD_GRAYSCALE)\n","\n","        if use_preprocessed:\n","            # Apply preprocessing to depth image\n","            depth_image = preprocess_depth_image(depth_image)\n","\n","        # Resize RGB to match depth dimensions\n","        depth_height, depth_width = depth_image.shape\n","        rgb_image_resized = cv2.resize(rgb_image, (depth_width, depth_height))\n","        rgb_image_rgb = cv2.cvtColor(rgb_image_resized, cv2.COLOR_BGR2RGB)\n","\n","        # Process with MediaPipe Hands\n","        results = hands.process(rgb_image_rgb)\n","        if not results.multi_hand_landmarks:\n","            continue\n","\n","        # Process each detected hand\n","        for hand_landmarks in results.multi_hand_landmarks:\n","            landmarks_3d = []\n","            for landmark in hand_landmarks.landmark:\n","                cx, cy = int(landmark.x * depth_width), int(landmark.y * depth_height)\n","                z = depth_image[cy, cx] if (0 <= cx < depth_width and 0 <= cy < depth_height) else 0\n","                landmarks_3d.append([cx, cy, z])\n","\n","            # Compute angles for each finger\n","            for finger, indices in finger_indices.items():\n","                if len(landmarks_3d) >= max(indices) + 1:\n","                    # Flexion Angles\n","                    mcp_angle = compute_vector_angle(landmarks_3d[indices[0]],\n","                                                     landmarks_3d[indices[1]],\n","                                                     landmarks_3d[indices[2]])\n","                    pip_angle = compute_vector_angle(landmarks_3d[indices[1]],\n","                                                     landmarks_3d[indices[2]],\n","                                                     landmarks_3d[indices[3]])\n","                    dip_angle = compute_vector_angle(landmarks_3d[indices[2]],\n","                                                     landmarks_3d[indices[3]],\n","                                                     landmarks_3d[indices[4]])\n","\n","                    # Abduction Distances (between adjacent fingertips)\n","                    if finger != \"thumb\":  # Thumb abduction is handled separately\n","                        prev_tip = landmarks_3d[finger_indices[finger][4] - 4]\n","                        current_tip = landmarks_3d[indices[4]]\n","                        abduction_distance = compute_abduction_distance(prev_tip, current_tip)\n","\n","                        print(f\"{finger.capitalize()} Finger Abduction Distance - Frame {i}: {abduction_distance:.2f} mm\")\n","\n","                    print(f\"{finger.capitalize()} Finger Angles - Frame {i}:\")\n","                    print(f\"  MCP: {mcp_angle:.2f}°\")\n","                    print(f\"  PIP: {pip_angle:.2f}°\")\n","                    print(f\"  DIP: {dip_angle:.2f}°\")\n","\n","            # Visualize the 3D Hand Pose\n","            visualize_3d_hand_pose(landmarks_3d, finger_indices, finger_colors, i)\n","\n","    hands.close()\n","\n","\n"]},{"cell_type":"code","source":["#RAW DEPTH DATA\n","\n","#preprocess depth images\n","def preprocess_depth_image(depth_image, min_depth=10, max_depth=1000, depth_scale=1.0):\n","    \"\"\"\n","    Preprocess depth image by applying filtering and normalization.\n","\n","    Parameters:\n","        depth_image (numpy.ndarray): Raw depth image in float32 format.\n","        min_depth (float): Minimum valid depth value.\n","        max_depth (float): Maximum valid depth value.\n","        depth_scale (float): Scale factor to convert raw depth units to millimeters.\n","\n","    Returns:\n","        numpy.ndarray: Preprocessed depth image.\n","    \"\"\"\n","    # Convert raw depth to millimeters using the scale factor\n","    depth_image_scaled = depth_image * depth_scale\n","\n","    # Remove noisy values outside the valid depth range\n","    depth_image_filtered = np.where((depth_image_scaled > min_depth) &\n","                                     (depth_image_scaled < max_depth), depth_image_scaled, 0)\n","\n","    # Apply median filtering to smooth noise\n","    depth_image_smoothed = cv2.medianBlur(depth_image_filtered.astype(np.uint16), ksize=5)\n","\n","    # Create a mask for missing regions\n","    mask = (depth_image_smoothed == 0).astype(np.uint8)\n","\n","    # Fill missing regions using inpainting\n","    depth_image_inpainted = cv2.inpaint(depth_image_smoothed.astype(np.uint8), mask,\n","                                        inpaintRadius=5, flags=cv2.INPAINT_TELEA)\n","\n","    # Apply bilateral filtering to refine edges\n","    depth_image_bilateral = cv2.bilateralFilter(depth_image_inpainted, d=9,\n","                                                sigmaColor=75, sigmaSpace=75)\n","\n","    # Normalize depth values to [0, 1] for consistent representation\n","    depth_image_normalized = cv2.normalize(depth_image_bilateral.astype(np.float32), None,\n","                                           alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n","\n","    return depth_image_normalized\n","\n","def match_rgb_and_depth_files(folder_path): #raw\n","    \"\"\"\n","    Match RGB and depth RAW files by corresponding numbers in filenames.\n","\n","    Parameters:\n","        folder_path (str): Path to the folder containing RGB and depth files.\n","\n","    Returns:\n","        dict: A dictionary where keys are numbers and values are dictionaries\n","              with 'rgb' and 'depth' file names.\n","    \"\"\"\n","    def extract_number(file_name):\n","        \"\"\"Extract number from file name using regex.\"\"\"\n","        match = re.search(r'_(\\d+)\\.', file_name)  # Matches '_<number>.'\n","        return int(match.group(1)) if match else None\n","\n","    # List and sort all files\n","    file_names = os.listdir(folder_path)\n","    rgb_files = sorted([f for f in file_names if f.startswith('rgbFrame')])\n","    depth_files = sorted([f for f in file_names if f.startswith('rawDepth')])\n","\n","    #print(f\"RGB Files: {rgb_files}\")\n","    #print(f\"Depth Files: {depth_files}\")\n","\n","    # Create a dictionary to match frames\n","    matched_frames = {}\n","\n","    # Populate the dictionary with RGB files\n","    for rgb_file in rgb_files:\n","        number = extract_number(rgb_file)\n","        #print(f\"Extracted number for RGB file {rgb_file}: {number}\")  # Debugging\n","        if number is not None:\n","            matched_frames[number] = {'rgb': rgb_file, 'depth': None}\n","\n","    # Match depth files to the same numbers\n","    for depth_file in depth_files:\n","        number = extract_number(depth_file)\n","        #print(f\"Extracted number for Depth file {depth_file}: {number}\")  # Debugging\n","        if number is not None and number in matched_frames:\n","            matched_frames[number]['depth'] = depth_file\n","\n","    # Filter unmatched pairs\n","    matched_frames = {k: v for k, v in matched_frames.items() if v['depth']}\n","    print(f\"Matched Frames: {matched_frames}\")  # Debugging\n","\n","    return matched_frames"],"metadata":{"id":"R9D8wSlDT19C","executionInfo":{"status":"ok","timestamp":1734556116933,"user_tz":300,"elapsed":441,"user":{"displayName":"Rima Rebei","userId":"16806725013394065055"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["# Main Execution\n","folder_path = \"/content/drive/MyDrive/cs-283-assignments/Final Project/hand_raw\"\n","finger_indices = {\n","    \"thumb\": [0, 1, 2, 3, 4],\n","    \"index\": [0, 5, 6, 7, 8],\n","    \"middle\": [0, 9, 10, 11, 12],\n","    \"ring\": [0, 13, 14, 15, 16],\n","    \"pinky\": [0, 17, 18, 19, 20],\n","}\n","finger_colors = {\n","    \"thumb\": \"red\",\n","    \"index\": \"green\",\n","    \"middle\": \"blue\",\n","    \"ring\": \"orange\",\n","    \"pinky\": \"purple\",\n","}\n","\n","matched_frames = match_rgb_and_depth_files(folder_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xAlnfqjc5R87","executionInfo":{"status":"ok","timestamp":1734556121035,"user_tz":300,"elapsed":641,"user":{"displayName":"Rima Rebei","userId":"16806725013394065055"}},"outputId":"4b25bf7d-b562-49a9-a9a2-6e4729006eb4"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Matched Frames: {0: {'rgb': 'rgbFrame_0.png', 'depth': 'rawDepth_0.raw'}, 1: {'rgb': 'rgbFrame_1.png', 'depth': 'rawDepth_1.raw'}, 10: {'rgb': 'rgbFrame_10.png', 'depth': 'rawDepth_10.raw'}, 11: {'rgb': 'rgbFrame_11.png', 'depth': 'rawDepth_11.raw'}, 12: {'rgb': 'rgbFrame_12.png', 'depth': 'rawDepth_12.raw'}, 13: {'rgb': 'rgbFrame_13.png', 'depth': 'rawDepth_13.raw'}, 14: {'rgb': 'rgbFrame_14.png', 'depth': 'rawDepth_14.raw'}, 15: {'rgb': 'rgbFrame_15.png', 'depth': 'rawDepth_15.raw'}, 16: {'rgb': 'rgbFrame_16.png', 'depth': 'rawDepth_16.raw'}, 17: {'rgb': 'rgbFrame_17.png', 'depth': 'rawDepth_17.raw'}, 18: {'rgb': 'rgbFrame_18.png', 'depth': 'rawDepth_18.raw'}, 19: {'rgb': 'rgbFrame_19.png', 'depth': 'rawDepth_19.raw'}, 2: {'rgb': 'rgbFrame_2.png', 'depth': 'rawDepth_2.raw'}, 20: {'rgb': 'rgbFrame_20.png', 'depth': 'rawDepth_20.raw'}, 21: {'rgb': 'rgbFrame_21.png', 'depth': 'rawDepth_21.raw'}, 22: {'rgb': 'rgbFrame_22.png', 'depth': 'rawDepth_22.raw'}, 23: {'rgb': 'rgbFrame_23.png', 'depth': 'rawDepth_23.raw'}, 24: {'rgb': 'rgbFrame_24.png', 'depth': 'rawDepth_24.raw'}, 25: {'rgb': 'rgbFrame_25.png', 'depth': 'rawDepth_25.raw'}, 26: {'rgb': 'rgbFrame_26.png', 'depth': 'rawDepth_26.raw'}, 27: {'rgb': 'rgbFrame_27.png', 'depth': 'rawDepth_27.raw'}, 28: {'rgb': 'rgbFrame_28.png', 'depth': 'rawDepth_28.raw'}, 29: {'rgb': 'rgbFrame_29.png', 'depth': 'rawDepth_29.raw'}, 3: {'rgb': 'rgbFrame_3.png', 'depth': 'rawDepth_3.raw'}, 30: {'rgb': 'rgbFrame_30.png', 'depth': 'rawDepth_30.raw'}, 31: {'rgb': 'rgbFrame_31.png', 'depth': 'rawDepth_31.raw'}, 32: {'rgb': 'rgbFrame_32.png', 'depth': 'rawDepth_32.raw'}, 33: {'rgb': 'rgbFrame_33.png', 'depth': 'rawDepth_33.raw'}, 34: {'rgb': 'rgbFrame_34.png', 'depth': 'rawDepth_34.raw'}, 35: {'rgb': 'rgbFrame_35.png', 'depth': 'rawDepth_35.raw'}, 36: {'rgb': 'rgbFrame_36.png', 'depth': 'rawDepth_36.raw'}, 37: {'rgb': 'rgbFrame_37.png', 'depth': 'rawDepth_37.raw'}, 38: {'rgb': 'rgbFrame_38.png', 'depth': 'rawDepth_38.raw'}, 39: {'rgb': 'rgbFrame_39.png', 'depth': 'rawDepth_39.raw'}, 4: {'rgb': 'rgbFrame_4.png', 'depth': 'rawDepth_4.raw'}, 40: {'rgb': 'rgbFrame_40.png', 'depth': 'rawDepth_40.raw'}, 41: {'rgb': 'rgbFrame_41.png', 'depth': 'rawDepth_41.raw'}, 42: {'rgb': 'rgbFrame_42.png', 'depth': 'rawDepth_42.raw'}, 43: {'rgb': 'rgbFrame_43.png', 'depth': 'rawDepth_43.raw'}, 44: {'rgb': 'rgbFrame_44.png', 'depth': 'rawDepth_44.raw'}, 45: {'rgb': 'rgbFrame_45.png', 'depth': 'rawDepth_45.raw'}, 46: {'rgb': 'rgbFrame_46.png', 'depth': 'rawDepth_46.raw'}, 47: {'rgb': 'rgbFrame_47.png', 'depth': 'rawDepth_47.raw'}, 48: {'rgb': 'rgbFrame_48.png', 'depth': 'rawDepth_48.raw'}, 49: {'rgb': 'rgbFrame_49.png', 'depth': 'rawDepth_49.raw'}, 5: {'rgb': 'rgbFrame_5.png', 'depth': 'rawDepth_5.raw'}, 50: {'rgb': 'rgbFrame_50.png', 'depth': 'rawDepth_50.raw'}, 51: {'rgb': 'rgbFrame_51.png', 'depth': 'rawDepth_51.raw'}, 52: {'rgb': 'rgbFrame_52.png', 'depth': 'rawDepth_52.raw'}, 53: {'rgb': 'rgbFrame_53.png', 'depth': 'rawDepth_53.raw'}, 54: {'rgb': 'rgbFrame_54.png', 'depth': 'rawDepth_54.raw'}, 55: {'rgb': 'rgbFrame_55.png', 'depth': 'rawDepth_55.raw'}, 56: {'rgb': 'rgbFrame_56.png', 'depth': 'rawDepth_56.raw'}, 57: {'rgb': 'rgbFrame_57.png', 'depth': 'rawDepth_57.raw'}, 58: {'rgb': 'rgbFrame_58.png', 'depth': 'rawDepth_58.raw'}, 59: {'rgb': 'rgbFrame_59.png', 'depth': 'rawDepth_59.raw'}, 6: {'rgb': 'rgbFrame_6.png', 'depth': 'rawDepth_6.raw'}, 60: {'rgb': 'rgbFrame_60.png', 'depth': 'rawDepth_60.raw'}, 61: {'rgb': 'rgbFrame_61.png', 'depth': 'rawDepth_61.raw'}, 62: {'rgb': 'rgbFrame_62.png', 'depth': 'rawDepth_62.raw'}, 63: {'rgb': 'rgbFrame_63.png', 'depth': 'rawDepth_63.raw'}, 64: {'rgb': 'rgbFrame_64.png', 'depth': 'rawDepth_64.raw'}, 65: {'rgb': 'rgbFrame_65.png', 'depth': 'rawDepth_65.raw'}, 66: {'rgb': 'rgbFrame_66.png', 'depth': 'rawDepth_66.raw'}, 67: {'rgb': 'rgbFrame_67.png', 'depth': 'rawDepth_67.raw'}, 68: {'rgb': 'rgbFrame_68.png', 'depth': 'rawDepth_68.raw'}, 69: {'rgb': 'rgbFrame_69.png', 'depth': 'rawDepth_69.raw'}, 7: {'rgb': 'rgbFrame_7.png', 'depth': 'rawDepth_7.raw'}, 70: {'rgb': 'rgbFrame_70.png', 'depth': 'rawDepth_70.raw'}, 71: {'rgb': 'rgbFrame_71.png', 'depth': 'rawDepth_71.raw'}, 72: {'rgb': 'rgbFrame_72.png', 'depth': 'rawDepth_72.raw'}, 73: {'rgb': 'rgbFrame_73.png', 'depth': 'rawDepth_73.raw'}, 74: {'rgb': 'rgbFrame_74.png', 'depth': 'rawDepth_74.raw'}, 75: {'rgb': 'rgbFrame_75.png', 'depth': 'rawDepth_75.raw'}, 76: {'rgb': 'rgbFrame_76.png', 'depth': 'rawDepth_76.raw'}, 77: {'rgb': 'rgbFrame_77.png', 'depth': 'rawDepth_77.raw'}, 78: {'rgb': 'rgbFrame_78.png', 'depth': 'rawDepth_78.raw'}, 79: {'rgb': 'rgbFrame_79.png', 'depth': 'rawDepth_79.raw'}, 8: {'rgb': 'rgbFrame_8.png', 'depth': 'rawDepth_8.raw'}, 80: {'rgb': 'rgbFrame_80.png', 'depth': 'rawDepth_80.raw'}, 81: {'rgb': 'rgbFrame_81.png', 'depth': 'rawDepth_81.raw'}, 82: {'rgb': 'rgbFrame_82.png', 'depth': 'rawDepth_82.raw'}, 83: {'rgb': 'rgbFrame_83.png', 'depth': 'rawDepth_83.raw'}, 84: {'rgb': 'rgbFrame_84.png', 'depth': 'rawDepth_84.raw'}, 85: {'rgb': 'rgbFrame_85.png', 'depth': 'rawDepth_85.raw'}, 86: {'rgb': 'rgbFrame_86.png', 'depth': 'rawDepth_86.raw'}, 87: {'rgb': 'rgbFrame_87.png', 'depth': 'rawDepth_87.raw'}, 9: {'rgb': 'rgbFrame_9.png', 'depth': 'rawDepth_9.raw'}}\n"]}]},{"cell_type":"code","source":["def visualize_3d_hand_pose_with_rgb(landmarks_3d, rgb_frame, frame_index):\n","    \"\"\"\n","    Visualize 3D hand pose alongside the corresponding RGB frame using matplotlib.\n","\n","    Parameters:\n","        landmarks_3d (list): List of 3D landmarks as [x, y, z].\n","        rgb_frame (numpy.ndarray): RGB frame as a numpy array (from cv2.imread or similar).\n","        frame_index (int): Current frame index for visualization.\n","    \"\"\"\n","    import matplotlib.pyplot as plt\n","\n","    # Ensure RGB frame is valid\n","    if rgb_frame is None or rgb_frame.size == 0:\n","        print(\"Error: RGB frame is empty or invalid.\")\n","        return\n","\n","    # External definitions of finger_indices and finger_colors\n","    global finger_indices, finger_colors\n","\n","    fig = plt.figure(figsize=(12, 6))\n","\n","    # Plot the RGB frame\n","    ax1 = fig.add_subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot\n","    try:\n","        ax1.imshow(cv2.cvtColor(rgb_frame, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for display\n","        ax1.set_title(f\"RGB Frame - Frame {frame_index}\")\n","        ax1.axis('off')  # Hide axis for the image\n","    except Exception as e:\n","        print(f\"Error displaying RGB frame: {e}\")\n","        return\n","\n","    # Plot the 3D hand pose\n","    ax2 = fig.add_subplot(1, 2, 2, projection='3d')  # 1 row, 2 columns, 2nd subplot\n","    try:\n","        xs = [pt[0] for pt in landmarks_3d]\n","        ys = [pt[1] for pt in landmarks_3d]\n","        zs = [pt[2] for pt in landmarks_3d]\n","\n","        # Plot all landmarks\n","        ax2.scatter(xs, ys, zs, c='black', s=20, label='Landmarks')\n","\n","        # Connect landmarks for each finger\n","        for finger, indices in finger_indices.items():\n","            finger_points = [landmarks_3d[i] for i in indices]\n","            x_finger = [pt[0] for pt in finger_points]\n","            y_finger = [pt[1] for pt in finger_points]\n","            z_finger = [pt[2] for pt in finger_points]\n","\n","            # Use predefined color for the finger\n","            color = finger_colors[finger]\n","            ax2.plot(x_finger, y_finger, z_finger, label=finger.capitalize(), color=color)\n","\n","        # Set axis labels and title\n","        ax2.set_xlabel('X (pixels)')\n","        ax2.set_ylabel('Y (pixels)')\n","        ax2.set_zlabel('Z (depth)')\n","        ax2.set_title(f\"3D Hand Pose - Frame {frame_index}\")\n","        ax2.legend()\n","    except Exception as e:\n","        print(f\"Error displaying 3D plot: {e}\")\n","        return\n","\n","    # Display both subplots\n","    plt.tight_layout()\n","    plt.show()\n","\n","\n","\n","\n","\n","def process_frames_and_debug_compute_angles(folder_path, matched_frames, finger_indices, finger_colors, use_preprocessed=True):\n","    \"\"\"\n","    Process RGB and depth frames, debug raw depth data loading, compute joint angles,\n","    and visualize 3D hand poses.\n","    \"\"\"\n","    mp_hands = mp.solutions.hands\n","    hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)\n","\n","    for i, (timestamp, files) in enumerate(matched_frames.items()):\n","        rgb_path = os.path.join(folder_path, files['rgb'])\n","        depth_path = os.path.join(folder_path, files['depth'])\n","\n","        print(f\"\\nProcessing Frame {i}: Timestamp {timestamp}\")\n","        print(f\"RGB Path: {rgb_path}\")\n","        print(f\"Depth Path: {depth_path}\")\n","\n","        # Load RGB and depth images\n","        rgb_image = cv2.imread(rgb_path)\n","        if rgb_image is None:\n","            print(f\"Error: Failed to load RGB image at {rgb_path}\")\n","            continue\n","\n","        try:\n","            with open(depth_path, 'rb') as f:\n","                depth_image = np.fromfile(f, dtype=np.float32)\n","            depth_image = depth_image.reshape((480, 640))  # Replace with actual dimensions\n","        except Exception as e:\n","            print(f\"Error loading depth file {depth_path}: {e}\")\n","            continue\n","\n","        if use_preprocessed:\n","            # Apply preprocessing to depth image\n","            try:\n","                depth_image = preprocess_depth_image(depth_image)\n","            except Exception as e:\n","                print(f\"Error preprocessing depth image: {e}\")\n","                continue\n","\n","        # Resize RGB to match depth dimensions\n","        depth_height, depth_width = depth_image.shape\n","        try:\n","            rgb_image_resized = cv2.resize(rgb_image, (depth_width, depth_height))\n","            rgb_image_rgb = cv2.cvtColor(rgb_image_resized, cv2.COLOR_BGR2RGB)\n","        except Exception as e:\n","            print(f\"Error resizing or converting RGB image: {e}\")\n","            continue\n","\n","        # Process with MediaPipe Hands\n","        results = hands.process(rgb_image_rgb)\n","        if not results.multi_hand_landmarks:\n","            print(\"No hands detected in this frame.\")\n","            continue\n","\n","        # Process each detected hand\n","        for hand_landmarks in results.multi_hand_landmarks:\n","            landmarks_3d = []\n","            for landmark in hand_landmarks.landmark:\n","                cx, cy = int(landmark.x * depth_width), int(landmark.y * depth_height)\n","                z = depth_image[cy, cx] if (0 <= cx < depth_width and 0 <= cy < depth_height) else 0\n","                landmarks_3d.append([cx, cy, z])\n","\n","            # Compute angles for each finger\n","            for finger, indices in finger_indices.items():\n","                if len(landmarks_3d) >= max(indices) + 1:\n","                    try:\n","                        # Flexion Angles\n","                        mcp_angle = compute_vector_angle(landmarks_3d[indices[0]],\n","                                                         landmarks_3d[indices[1]],\n","                                                         landmarks_3d[indices[2]])\n","                        pip_angle = compute_vector_angle(landmarks_3d[indices[1]],\n","                                                         landmarks_3d[indices[2]],\n","                                                         landmarks_3d[indices[3]])\n","                        dip_angle = compute_vector_angle(landmarks_3d[indices[2]],\n","                                                         landmarks_3d[indices[3]],\n","                                                         landmarks_3d[indices[4]])\n","\n","                        print(f\"{finger.capitalize()} Finger Angles - Frame {i}:\")\n","                        print(f\"  MCP: {mcp_angle:.2f}°\")\n","                        print(f\"  PIP: {pip_angle:.2f}°\")\n","                        print(f\"  DIP: {dip_angle:.2f}°\")\n","\n","                        # Abduction Distances\n","                        if finger != \"thumb\":  # Thumb abduction is handled separately\n","                            prev_tip = landmarks_3d[finger_indices[finger][4] - 4]\n","                            current_tip = landmarks_3d[indices[4]]\n","                            abduction_distance = compute_abduction_distance(prev_tip, current_tip)\n","                            print(f\"{finger.capitalize()} Finger Abduction Distance - Frame {i}: {abduction_distance:.2f} mm\")\n","                    except Exception as e:\n","                        print(f\"Error computing angles or distances for {finger}: {e}\")\n","\n","            # Visualize the 3D Hand Pose\n","            try:\n","                visualize_3d_hand_pose(landmarks_3d, rgb_image_rgb, i)\n","            except Exception as e:\n","                print(f\"Error visualizing 3D hand pose: {e}\")\n","\n","    hands.close()\n"],"metadata":{"id":"w6JuaxaIU70g","executionInfo":{"status":"ok","timestamp":1734559519102,"user_tz":300,"elapsed":592,"user":{"displayName":"Rima Rebei","userId":"16806725013394065055"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["process_frames_and_debug_compute_angles(folder_path, matched_frames, finger_indices, finger_colors, use_preprocessed=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1BN_muyUgX6CJnSYV-_2qf0TPztvCoGF-"},"id":"6qkBO1KrQivs","executionInfo":{"status":"ok","timestamp":1734556184129,"user_tz":300,"elapsed":56650,"user":{"displayName":"Rima Rebei","userId":"16806725013394065055"}},"outputId":"e29eb368-de3d-4e88-806a-669b7cb17aa0"},"execution_count":43,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["#Participant 1\n","participant_1_path = '/content/drive/MyDrive/cs-283-assignments/Final Project/hand_participant'\n","process_frames_and_debug_compute_angles(participant_1_path, matched_frames, finger_indices, finger_colors, use_preprocessed=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1TiSvcxLmZD6nPS-B6ZHkemsABJvFfDLa"},"id":"QPKLL9mGx76J","executionInfo":{"status":"ok","timestamp":1734559704627,"user_tz":300,"elapsed":182768,"user":{"displayName":"Rima Rebei","userId":"16806725013394065055"}},"outputId":"322b3fc4-43dc-40a5-bca6-5ec76b9f5518"},"execution_count":57,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["## 4 Poses Classifier"],"metadata":{"id":"XITPHWV3ryCA"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","file_path = \"/content/drive/MyDrive/cs-283-assignments/Final Project/Pose_Dataset.csv\"\n","df = pd.read_csv(file_path)\n","df.head()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":348},"id":"bz6nQQcurw2f","executionInfo":{"status":"ok","timestamp":1734560444092,"user_tz":300,"elapsed":1005,"user":{"displayName":"Rima Rebei","userId":"16806725013394065055"}},"outputId":"40e8853e-5d28-4a48-e7b1-aa04ab581710"},"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Frame  Thumb MCP  Thumb PIP  Thumb DIP  Index MCP  Index PIP  Index DIP  \\\n","0      6      24.73       0.32       8.81       1.31       0.66       1.44   \n","1      7      25.11       0.64      11.50       2.49       1.68       2.69   \n","2     12      26.15       1.17       8.81       1.46       1.80       4.39   \n","3     16      34.53       7.16       9.10       2.88       2.14       2.22   \n","4     10      38.19      24.31      11.50      14.86       5.19       2.90   \n","\n","   Index Abduction  Middle MCP  Middle PIP  ...  Middle Abduction  Ring MCP  \\\n","0           118.53        7.15        0.24  ...             65.92      5.36   \n","1           118.12        8.29        0.36  ...             64.47      4.42   \n","2           122.67        8.22        0.85  ...             62.30      3.18   \n","3            98.90        1.80        0.53  ...             46.52      7.76   \n","4            86.41        3.54        0.00  ...             24.04     11.36   \n","\n","   Ring PIP  Ring DIP  Ring Abduction  Pinky MCP  Pinky PIP  Pinky DIP  \\\n","0      2.68      0.42           28.84       4.66       0.16       2.53   \n","1      2.46      0.32           29.15       5.52       0.80       1.89   \n","2      2.94      1.38           29.68       4.66       1.55       5.08   \n","3      3.43      0.00           32.65       0.11       2.05       1.01   \n","4      2.30      0.77           24.08      26.88       4.40       1.91   \n","\n","   Pinky Abduction    Pose  \n","0            83.63  Pose 1  \n","1            83.55  Pose 1  \n","2            80.72  Pose 1  \n","3            79.40  Pose 1  \n","4            44.42  Pose 2  \n","\n","[5 rows x 21 columns]"],"text/html":["\n","  <div id=\"df-d9fd1ead-6188-45d5-8255-06a9ff76beeb\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Frame</th>\n","      <th>Thumb MCP</th>\n","      <th>Thumb PIP</th>\n","      <th>Thumb DIP</th>\n","      <th>Index MCP</th>\n","      <th>Index PIP</th>\n","      <th>Index DIP</th>\n","      <th>Index Abduction</th>\n","      <th>Middle MCP</th>\n","      <th>Middle PIP</th>\n","      <th>...</th>\n","      <th>Middle Abduction</th>\n","      <th>Ring MCP</th>\n","      <th>Ring PIP</th>\n","      <th>Ring DIP</th>\n","      <th>Ring Abduction</th>\n","      <th>Pinky MCP</th>\n","      <th>Pinky PIP</th>\n","      <th>Pinky DIP</th>\n","      <th>Pinky Abduction</th>\n","      <th>Pose</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6</td>\n","      <td>24.73</td>\n","      <td>0.32</td>\n","      <td>8.81</td>\n","      <td>1.31</td>\n","      <td>0.66</td>\n","      <td>1.44</td>\n","      <td>118.53</td>\n","      <td>7.15</td>\n","      <td>0.24</td>\n","      <td>...</td>\n","      <td>65.92</td>\n","      <td>5.36</td>\n","      <td>2.68</td>\n","      <td>0.42</td>\n","      <td>28.84</td>\n","      <td>4.66</td>\n","      <td>0.16</td>\n","      <td>2.53</td>\n","      <td>83.63</td>\n","      <td>Pose 1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7</td>\n","      <td>25.11</td>\n","      <td>0.64</td>\n","      <td>11.50</td>\n","      <td>2.49</td>\n","      <td>1.68</td>\n","      <td>2.69</td>\n","      <td>118.12</td>\n","      <td>8.29</td>\n","      <td>0.36</td>\n","      <td>...</td>\n","      <td>64.47</td>\n","      <td>4.42</td>\n","      <td>2.46</td>\n","      <td>0.32</td>\n","      <td>29.15</td>\n","      <td>5.52</td>\n","      <td>0.80</td>\n","      <td>1.89</td>\n","      <td>83.55</td>\n","      <td>Pose 1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>12</td>\n","      <td>26.15</td>\n","      <td>1.17</td>\n","      <td>8.81</td>\n","      <td>1.46</td>\n","      <td>1.80</td>\n","      <td>4.39</td>\n","      <td>122.67</td>\n","      <td>8.22</td>\n","      <td>0.85</td>\n","      <td>...</td>\n","      <td>62.30</td>\n","      <td>3.18</td>\n","      <td>2.94</td>\n","      <td>1.38</td>\n","      <td>29.68</td>\n","      <td>4.66</td>\n","      <td>1.55</td>\n","      <td>5.08</td>\n","      <td>80.72</td>\n","      <td>Pose 1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>16</td>\n","      <td>34.53</td>\n","      <td>7.16</td>\n","      <td>9.10</td>\n","      <td>2.88</td>\n","      <td>2.14</td>\n","      <td>2.22</td>\n","      <td>98.90</td>\n","      <td>1.80</td>\n","      <td>0.53</td>\n","      <td>...</td>\n","      <td>46.52</td>\n","      <td>7.76</td>\n","      <td>3.43</td>\n","      <td>0.00</td>\n","      <td>32.65</td>\n","      <td>0.11</td>\n","      <td>2.05</td>\n","      <td>1.01</td>\n","      <td>79.40</td>\n","      <td>Pose 1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10</td>\n","      <td>38.19</td>\n","      <td>24.31</td>\n","      <td>11.50</td>\n","      <td>14.86</td>\n","      <td>5.19</td>\n","      <td>2.90</td>\n","      <td>86.41</td>\n","      <td>3.54</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>24.04</td>\n","      <td>11.36</td>\n","      <td>2.30</td>\n","      <td>0.77</td>\n","      <td>24.08</td>\n","      <td>26.88</td>\n","      <td>4.40</td>\n","      <td>1.91</td>\n","      <td>44.42</td>\n","      <td>Pose 2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 21 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d9fd1ead-6188-45d5-8255-06a9ff76beeb')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d9fd1ead-6188-45d5-8255-06a9ff76beeb button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d9fd1ead-6188-45d5-8255-06a9ff76beeb');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-7a184966-cf5a-4f0a-93fc-ffc6a10144c0\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7a184966-cf5a-4f0a-93fc-ffc6a10144c0')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-7a184966-cf5a-4f0a-93fc-ffc6a10144c0 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df"}},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["#normalize features (exclude 'Frame' and 'Pose')\n","scaler = MinMaxScaler()\n","feature_columns = df.columns[1:-1]  # All columns except 'Frame' and 'Pose'\n","df[feature_columns] = scaler.fit_transform(df[feature_columns])\n","\n","def augment_data(df, num_augmentations=10, angle_variation=0.05):\n","    augmented_data = []\n","\n","    for _, row in df.iterrows():\n","        base_features = row[feature_columns].values\n","        label = row[\"Pose\"]\n","\n","        for _ in range(num_augmentations):\n","            # Add random noise to features\n","            augmented_features = base_features + np.random.uniform(\n","                -angle_variation, angle_variation, size=base_features.shape\n","            )\n","            # Clamp values to [0, 1] range\n","            augmented_features = np.clip(augmented_features, 0, 1)\n","            augmented_data.append(np.append(augmented_features, label))\n","\n","    augmented_columns = feature_columns.tolist() + [\"Pose\"]\n","    augmented_df = pd.DataFrame(augmented_data, columns=augmented_columns)\n","    return augmented_df\n","\n","#augment the dataset\n","augmented_df = augment_data(df, num_augmentations=10)\n","print(\"Augmented Dataset:\")\n","print(augmented_df.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GN5c3wIZr2ta","executionInfo":{"status":"ok","timestamp":1734560493172,"user_tz":300,"elapsed":573,"user":{"displayName":"Rima Rebei","userId":"16806725013394065055"}},"outputId":"838479a7-4209-497e-faf4-c20ca30fe1ec"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["Augmented Dataset:\n","   Thumb MCP  Thumb PIP  Thumb DIP  Index MCP  Index PIP  Index DIP  \\\n","0   0.004839   0.000000   0.728602   0.014265   0.020271   0.000000   \n","1   0.001191   0.000000   0.705886   0.000000   0.000000   0.012805   \n","2   0.000000   0.000000   0.706351   0.000453   0.036048   0.004728   \n","3   0.048822   0.000000   0.679012   0.041850   0.000000   0.000000   \n","4   0.022434   0.037997   0.689007   0.049638   0.000000   0.043351   \n","\n","   Index Abduction  Middle MCP  Middle PIP  Middle DIP  Middle Abduction  \\\n","0         0.938172    0.051369    0.048547     0.00000          0.991247   \n","1         0.960543    0.073758    0.008147     0.04929          0.952921   \n","2         0.985342    0.004466    0.000000     0.00000          0.958856   \n","3         0.993318    0.094775    0.008551     0.00816          1.000000   \n","4         0.984982    0.033486    0.000000     0.00000          0.975475   \n","\n","   Ring MCP  Ring PIP  Ring DIP  Ring Abduction  Pinky MCP  Pinky PIP  \\\n","0  0.054117  0.000000  0.000000        0.834457   0.061380   0.000000   \n","1  0.020245  0.061551  0.000000        0.822850   0.067415   0.000000   \n","2  0.068569  0.000000  0.040102        0.808359   0.000000   0.020784   \n","3  0.065481  0.035930  0.016082        0.872807   0.000000   0.009822   \n","4  0.045513  0.000000  0.018919        0.847643   0.031122   0.000000   \n","\n","   Pinky DIP  Pinky Abduction    Pose  \n","0   0.073420         1.000000  Pose 1  \n","1   0.041803         1.000000  Pose 1  \n","2   0.011108         0.955875  Pose 1  \n","3   0.041891         0.981164  Pose 1  \n","4   0.084452         0.967671  Pose 1  \n"]}]},{"cell_type":"code","source":["# need more data ---> but can now use this for real time app pose classification\n","#to make sure users are doing correct poses so we can properly track progress\n","\n","# split the dataset into features and labels\n","X = augmented_df[feature_columns].values\n","y = augmented_df[\"Pose\"].values\n","\n","#train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","#train Random Forest classifier\n","clf = RandomForestClassifier(random_state=42)\n","clf.fit(X_train, y_train)\n","\n","#evaluate the classifier ---> to use once i add more data\n","y_pred = clf.predict(X_test)\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mILmFGFUr7no","executionInfo":{"status":"ok","timestamp":1734560581860,"user_tz":300,"elapsed":1451,"user":{"displayName":"Rima Rebei","userId":"16806725013394065055"}},"outputId":"bd13ef06-3c22-40d9-bf9d-bc97dbc50c91"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["Classification Report:\n","              precision    recall  f1-score   support\n","\n","      Pose 1       1.00      1.00      1.00         2\n","      Pose 3       1.00      1.00      1.00         2\n","      Pose 4       1.00      1.00      1.00         2\n","\n","    accuracy                           1.00         6\n","   macro avg       1.00      1.00      1.00         6\n","weighted avg       1.00      1.00      1.00         6\n","\n","Confusion Matrix:\n","[[2 0 0]\n"," [0 2 0]\n"," [0 0 2]]\n"]}]},{"cell_type":"code","source":["from zipfile import ZipFile\n","%cd \"/content/drive/My Drive/cs-283-assignments/Final Project\"\n","\n","def writeFolder(zip, path):\n","  import os\n","  for dirpath,dirs,files in os.walk(path):\n","    for f in files:\n","      fn = os.path.join(dirpath, f)\n","      zip.write(fn)\n","\n","def writeFile(zip, path):\n","  zip.write(path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JAM5OqT14X1p","executionInfo":{"status":"ok","timestamp":1734560911621,"user_tz":300,"elapsed":417,"user":{"displayName":"Rima Rebei","userId":"16806725013394065055"}},"outputId":"9ede8b46-07bb-4813-eabd-61a27cb71471"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/cs-283-assignments/Final Project\n"]}]},{"cell_type":"code","source":["# Zip source\n","zipObj = ZipFile('REBEI_RIMA_final_project.zip', 'w')\n","\n","writeFolder(zipObj, \"CV_Final_Project\")\n","writeFile(zipObj, \"CV_Final_Project.ipynb\")\n","\n","zipObj.close()"],"metadata":{"id":"mK50p0kw4aWU","executionInfo":{"status":"ok","timestamp":1734560915076,"user_tz":300,"elapsed":843,"user":{"displayName":"Rima Rebei","userId":"16806725013394065055"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["!jupyter nbconvert --to html CV_Final_Project.ipynb --output \"REBEI_RIMA_final_project.html\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fbMfURpC5HXv","executionInfo":{"status":"ok","timestamp":1734561043919,"user_tz":300,"elapsed":6756,"user":{"displayName":"Rima Rebei","userId":"16806725013394065055"}},"outputId":"77cebbe7-2013-4afb-d53c-face5ae00940"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["[NbConvertApp] Converting notebook CV_Final_Project.ipynb to html\n","[NbConvertApp] Writing 391793 bytes to REBEI_RIMA_final_project.html\n"]}]}]}